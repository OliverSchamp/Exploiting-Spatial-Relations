"""
Train RetinaNet. Can be run from the command line or from IDE. If running from IDE, change the dataset file for different data trainings
"""

import torch
import torchvision
import os
# import transforms as T
# import utils
import matplotlib.pyplot as plt
import math
import time
import numpy as np
from torchvision import  transforms
import torch.utils
from torchvision.utils import make_grid,draw_bounding_boxes
from engine import train_one_epoch, evaluate
from torchvision.models.detection import roi_heads
# from datasets.coco_eval import CocoEvaluator
from coco_eval import CocoEvaluator
import coco_eval
from datasets import build_dataset, get_coco_api_from_dataset
import argparse
print(torch.__version__)
print(torchvision.__version__)

import util.misc as utils

from torch.utils.data import DataLoader, DistributedSampler, RandomSampler , BatchSampler

from pathlib import Path

import cv2

def get_args_parser():
    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)
    parser.add_argument('--batch_size', default=1, type=int)

    # * Segmentation
    parser.add_argument('--masks', action='store_true',
                        help="Train segmentation head if the flag is provided")
    # dataset parameters
    parser.add_argument('--dataset_file', default='foos_positional') # CHANGE THIS TO CHANGE THE DATA TRAINED UPON
    parser.add_argument('--data_path', default='C:/Users/OliverSchamp/Documents/Thesis/Segmentsai/', type=str) #changed from --coco_path
    parser.add_argument('--output_dir', default='retinanet_output/',
                        help='path where to save, empty for no saving')
    parser.add_argument('--device', default='cuda',
                        help='device to use for training / testing')
    parser.add_argument('--seed', default=42, type=int)
    parser.add_argument('--num_workers', default=2, type=int)

    return parser

def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(0)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=0)

def rescale_bboxes(out_bbox, size):
    img_w, img_h = size
    # img_w, img_h = (1280, 720)
    b = box_cxcywh_to_xyxy(out_bbox)
    b = b * torch.tensor([img_w, img_h,
                          img_w, img_h
                          ], dtype=torch.float32)
    return b

def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):
    model.train()
    metric_logger = utils.MetricLogger(delimiter="  ")
    metric_logger.add_meter("lr", utils.SmoothedValue(window_size=1, fmt="{value:.6f}"))
    header = f"Epoch: [{epoch}]"   
    lr_scheduler = None
    if epoch == 0:
        warmup_factor = 1.0 / 1000
        warmup_iters = min(1000, len(data_loader) - 1)

        lr_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer, start_factor=warmup_factor, total_iters=warmup_iters
        )

    batch_losses = []

    for images, targets in metric_logger.log_every(data_loader, print_freq, header):
        images, _ = images.decompose()
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # resize the boxes in targets to match what retinanet expects, as the dataloader comes from DETR
        for idxx, _ in enumerate(targets[0]['boxes']):
            targets[0]['boxes'][idxx][0] *= targets[0]['size'][1]
            targets[0]['boxes'][idxx][1] *= targets[0]['size'][0]
            targets[0]['boxes'][idxx][2] *= targets[0]['size'][1]
            targets[0]['boxes'][idxx][3] *= targets[0]['size'][0]

            targets[0]['boxes'][idxx] = box_cxcywh_to_xyxy(targets[0]['boxes'][idxx])

        targets[0]['labels'] += 1

        min_val = torch.min(images[0])
        max_val = torch.max(images[0])

        images[0] = (images[0] - min_val) * (1/(max_val-min_val))

        # drawResults(images, targets)

        with torch.cuda.amp.autocast(enabled=scaler is not None):
            loss_dict = model(images, targets)
            #print(loss_dict)
            losses = sum(loss for loss in loss_dict.values())

        # reduce losses over all GPUs for logging purposes
        loss_dict_reduced = utils.reduce_dict(loss_dict)
        losses_reduced = sum(loss for loss in loss_dict_reduced.values())
   
        loss_value = losses_reduced.item()

        if not math.isfinite(loss_value):
            print(f"Loss is {loss_value}, stopping training")
           
            #sys.exit(1)

        optimizer.zero_grad()
        if scaler is not None:
            scaler.scale(losses).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            losses.backward()
            optimizer.step()

        if lr_scheduler is not None:
            lr_scheduler.step()

        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)
        metric_logger.update(lr=optimizer.param_groups[0]["lr"])
        batch_losses.append(losses_reduced.cpu().detach().numpy())

    return metric_logger, batch_losses
    

import csv

@torch.inference_mode()
def evaluate(model, data_loader, device, threshold,post):
    cpu_device = torch.device("cpu")
    model.eval()
    metric_logger = utils.MetricLogger(delimiter="  ")
    header = "Test:"
    coco = get_coco_api_from_dataset(data_loader.dataset)
    iou_types = ["bbox"]
    coco_evaluator = CocoEvaluator(coco, iou_types)
    count = 0
    for images, targets in metric_logger.log_every(data_loader, 100, header):
        images, _ = images.decompose()

        images = list(img.to(device) for img in images)

        # resize the boxes in targets to integer coordinates
        for idxx, _ in enumerate(targets[0]['boxes']):
            targets[0]['boxes'][idxx][0] *= targets[0]['size'][1]
            targets[0]['boxes'][idxx][1] *= targets[0]['size'][0]
            targets[0]['boxes'][idxx][2] *= targets[0]['size'][1]
            targets[0]['boxes'][idxx][3] *= targets[0]['size'][0]

            targets[0]['boxes'][idxx] = box_cxcywh_to_xyxy(targets[0]['boxes'][idxx])
        targets[0]['labels'] += 1

        min_val = torch.min(images[0])
        max_val = torch.max(images[0])

        images[0] = (images[0] - min_val) * (1 / (max_val - min_val))

        if torch.cuda.is_available():
            torch.cuda.synchronize()
        model_time = time.time()
        outputs = model(images)
        outputs = filterOutput(outputs,threshold)
        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]
        model_time = time.time() - model_time

        #visualising the first image targets and the first image outputs
        # outputs_visual = outputs
        # outputs_visual[0]['labels'][outputs_visual[0]['labels'] > 4] = 0
        # if torch.all(outputs_visual[0]['labels'] < 5) and count == 0:
        #     drawTargets(images, targets)
        #     drawResults(images, outputs_visual)

        #reformat outputs like in DETR version: 0:ball, 1:figure, 2:goal, 3:table
        # essentially do the inverse of the targets preprocessing
        # resize the boxes in targets to integer coordinates
        for idxx, _ in enumerate(outputs[0]['boxes']):
            outputs[0]['boxes'][idxx][0] *= targets[0]['orig_size'][1]/targets[0]['size'][1]
            outputs[0]['boxes'][idxx][1] *= targets[0]['orig_size'][0]/targets[0]['size'][0]
            outputs[0]['boxes'][idxx][2] *= targets[0]['orig_size'][1]/targets[0]['size'][1]
            outputs[0]['boxes'][idxx][3] *= targets[0]['orig_size'][0]/targets[0]['size'][0]
        outputs[0]['labels'] = outputs[0]['labels'] - 1

        res = {target["image_id"].item(): output for target, output in zip(targets, outputs)}

        evaluator_time = time.time()
        coco_evaluator.update(res)
        evaluator_time = time.time() - evaluator_time
        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)

        count += 1

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    #print("Averaged stats:", metric_logger)
    coco_evaluator.synchronize_between_processes()

    # accumulate predictions from all images
    coco_evaluator.accumulate()
    d = coco_evaluator.summarize()

    return coco_evaluator, d

#function for drawing the annotations of RetinaNet
def drawResults(imageList,outputs,bars = None):
    # classes =  ['__background__', 'playing field','goal','player','ball' ]
    classes = ['__background__', 'ball', 'figure', 'goal', 'table']
    colors = ['black','yellow','orange','blue','red']
    threshold =0
    
    for image, output in zip(imageList,outputs):
            boxes = output['boxes'][output['scores'] > threshold]
            labelsList = output['labels'][output['scores']> threshold].tolist()
            colorsList = output['labels'][output['scores'] > threshold].tolist()
            labels = [classes[int(i)] for i in labelsList]
            colors = [colors[int(i)] for i in colorsList]
            f =plt.figure(figsize =(20,20))          
            plt.rcParams["savefig.bbox"] = 'tight'
            result = [draw_bounding_boxes((image*255).to(torch.uint8), boxes=boxes ,labels=labels, colors =colors, width=5,font_size=3) for image, output in zip(imageList, outputs)]
            
           
            if bars != None:
                #print("drawing bars")
                for bar in bars:
                        xList = []
                        yList = []
                        for figure in bar:
                            xList.append(float(figure[0][0] + figure[0][2]) / 2)
                            yList.append(float(figure[0][1] + figure[0][3]) / 2)
                        visibility =[1]
                        connectivity = []
                        for i in range(len(xList)-1):
                            visibility.append(1)
                            connectivity.append((i,i+1))
                        keypoints = [[list(a) for a in zip(xList,yList,visibility)]]
                        keypoints = torch.Tensor(keypoints)
                        #print(result[0])
                        result = [torchvision.utils.draw_keypoints(result[0],keypoints,connectivity=connectivity, radius=4, colors=("blue"),width=5)]

            # plt.imshow(result[0].permute(1, 2, 0).to(torch.uint8))
            plt.imshow(result[0].permute(1, 2, 0).to(torch.uint8))

            # plt.imshow(result[0].permute(1, 2, 0).to(torch.float32)/255)
            f.savefig('t.png')
            plt.show()  
           
    return

# function for drawing the hand-labelled annotations
def drawTargets(imageList, outputs, bars=None):
    # classes =  ['__background__', 'playing field','goal','player','ball' ]
    classes = ['__background__', 'ball', 'figure', 'goal', 'table']
    colors = ['black', 'yellow', 'orange', 'blue', 'red']
    threshold = 0

    for image, output in zip(imageList, outputs):
        # print(output)
        boxes = output['boxes']#[output['scores'] > threshold]
        labelsList = output['labels'].tolist()#[output['scores'] > threshold].tolist()
        colorsList = output['labels'].tolist()#[output['scores'] > threshold].tolist()
        labels = [classes[int(i)] for i in labelsList]
        colors = [colors[int(i)] for i in colorsList]
        f = plt.figure(figsize=(20, 20))
        plt.rcParams["savefig.bbox"] = 'tight'
        result = [draw_bounding_boxes((image * 255).to(torch.uint8), boxes=boxes, labels=labels, colors=colors, width=5,
                                      font_size=3) for image, output in zip(imageList, outputs)]

        if bars != None:
            # print("drawing bars")
            for bar in bars:
                xList = []
                yList = []
                for figure in bar:
                    xList.append(float(figure[0][0] + figure[0][2]) / 2)
                    yList.append(float(figure[0][1] + figure[0][3]) / 2)
                visibility = [1]
                connectivity = []
                for i in range(len(xList) - 1):
                    visibility.append(1)
                    connectivity.append((i, i + 1))
                keypoints = [[list(a) for a in zip(xList, yList, visibility)]]
                keypoints = torch.Tensor(keypoints)
                # print(result[0])
                result = [torchvision.utils.draw_keypoints(result[0], keypoints, connectivity=connectivity, radius=4,
                                                           colors=("blue"), width=5)]

        # plt.imshow(result[0].permute(1, 2, 0).to(torch.uint8))
        plt.imshow(result[0].permute(1, 2, 0).to(torch.uint8))

        # plt.imshow(result[0].permute(1, 2, 0).to(torch.float32)/255)
        f.savefig('t.png')
        plt.show()

    return
    
def filterOutput(outputs,threshold):
    output = outputs[0]
    newDict = {}           
    for key in output:
        newDict[key] = output[key][output['scores'] >= threshold]
    return [newDict]

def average_of_10_numbers(numbers):
    output = []
    length = len(numbers)

    for i in range(0, length, 10):
        average = sum(numbers[i:i+10]) / 10.0
        output.append(average)

    return output

def main(args):
    dataset_train = build_dataset(image_set='train', args=args)  # calls the build_dataset function in foos.py
    dataset_test = build_dataset(image_set='val', args=args)
    sampler_train = torch.utils.data.RandomSampler(dataset_train) #create dataloaders - random sampling of train set
    sampler_val = torch.utils.data.SequentialSampler(dataset_test)

    batch_sampler_train = torch.utils.data.BatchSampler(
        sampler_train, args.batch_size, drop_last=True) #batch training images during training

    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,
                                   collate_fn=utils.collate_fn, num_workers=args.num_workers)  # collate_fn - misc.py
    data_loader_test = DataLoader(dataset_test, args.batch_size, sampler=sampler_val,
                                 drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)

    learning_rates = 0.001
    momentum = 0.9
    weight_decay =0.0001
    results = []

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=learning_rates,momentum=momentum, weight_decay=weight_decay)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.1)
    """
    checkpoint = torch.load('/kaggle/input/models-thesis/all.tar', map_location ="cuda:0")
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    """

    model.to(device)
    num_epochs = 15
    file_train = open(args.output_dir + 'train_pos.csv', 'w')
    writer_train = csv.writer(file_train, delimiter=',')
    total_losses = []
    for epoch in range(num_epochs):
        # train for one epoch, printing every 10 iterations
        logs, losses = train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=36)
        losses = average_of_10_numbers(losses)
        total_losses.append(losses)
        #evaluate
        print("EVALUATING ON VALIDATION TEST")
        evaluatorr, res = evaluate(model, data_loader_test, device=device, threshold=0.5, post=False)
        results.append(res)
        # update the learning rate
        lr_scheduler.step()

    total_losses = [[element for sublist in total_losses for element in sublist]]
    writer_train.writerows(total_losses)
    file_train.close()
    file = open(args.output_dir + 'val_pos.csv', 'w')
    writer = csv.writer(file,delimiter  =',')
    header = ['MAP 0.5:95 all 100','MAP 0.5 all 100','MAP 0.75 all 100','MAP 0.5:95 small 100','MAP 0.5:95 medium 100','MAP 0.5:95 large 100','Recall 0.5:95 all 1','Recall 0.5:95 all 10','Recall 0.5:95 all 100','Recall 0.5:95 small 100','Recall 0.5:95 medium 1','Recall 0.5:95 large 100']
    writer.writerow(header)
    writer.writerows(results)
    file.close()

    model_scripted = torch.jit.script(model)  # Export to TorchScript
    model_scripted.save(args.output_dir + 'model_scripted.pt')  # Save

if __name__ == '__main__':
    parser = argparse.ArgumentParser('RetinaNet training and evaluation script', parents=[get_args_parser()])
    args = parser.parse_args()
    if args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    main(args)