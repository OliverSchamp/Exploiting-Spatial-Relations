"""
CODE FOR TESTING RETINANET, AND ALSO VISUALISING THE TARGETS OF A DATASET. SEE ALSO retinanet_visual.py

On Validation Set, 4 classes:
python RetinaNet/test_retinanet.py --data_path ../Segmentsai/Test --resume RetinaNet/retinanet_output/model_scripted_original.pt --dataset_file foos --thresh 0.7 --output_dir images/ --image_set val
On Synthetic data, 4 classes:
python RetinaNet/test_retinanet.py --data_path ../Segmentsai/synthesiser/data --resume RetinaNet/retinanet_output/model_scripted_original.pt --dataset_file foos --thresh 0.5 --output_dir RetinaNet/test_images/ --image_set test

On Validation set, 14 classes:
python RetinaNet/test_retinanet.py --data_path ../Segmentsai/Test --resume RetinaNet/retinanet_output/model_scripted.pt --dataset_file foos_positional --thresh 0.5 --output_dir RetinaNet/images_pos/ --image_set val --nclasses 14


"""

import torch
import torchvision
import matplotlib.pyplot as plt
import time
import torch.utils
from torchvision.utils import draw_bounding_boxes
from coco_eval import CocoEvaluator
import argparse
from torch.utils.data import DataLoader, DistributedSampler, RandomSampler, BatchSampler
from pathlib import Path
import csv
import colorsys
import random
import os
import sys
import numpy as np

filedir = os.path.join("..", os.getcwd())
sys.path.append(filedir)
from datasets import build_dataset, get_coco_api_from_dataset
import util.misc as utils

def create_distinct_colors(n):
    colors = []
    for i in range(n):
        hue = i / n  # Vary the hue value evenly
        saturation = 0.7  # Adjust the saturation and lightness as needed
        lightness = 0.6
        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)
        rgb_scaled = tuple(int(x * 255) for x in rgb)  # Scale RGB values to 0-255
        colors.append(rgb_scaled)
    return colors


def get_args_parser():
    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)
    parser.add_argument('--batch_size', default=1, type=int)

    # * Segmentation
    parser.add_argument('--masks', action='store_true',
                        help="Train segmentation head if the flag is provided")
    # dataset parameters
    parser.add_argument('--dataset_file', default='foos_positional')
    parser.add_argument('--data_path', default='C:/Users/OliverSchamp/Documents/Thesis/Segmentsai/',
                        type=str)  # changed from --coco_path
    parser.add_argument('--output_dir', default='retinanet_output/', type=str,
                        help='path where to save, empty for no saving')
    parser.add_argument('--device', default='cuda',
                        help='device to use for training / testing')
    parser.add_argument('--seed', default=42, type=int)
    parser.add_argument('--num_workers', default=2, type=int)

    parser.add_argument('--nclasses', default=4, type=int)
    parser.add_argument('--image_set', default='', type=str)
    parser.add_argument('--thresh', default=0.7, type=float)
    parser.add_argument('--resume', default='', help='resume from checkpoint')

    return parser


def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(0)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=0)


@torch.inference_mode()
def evaluate(model, data_loader, device, threshold, output_dir, nclasses):
    cpu_device = torch.device("cpu")
    model.eval()
    metric_logger = utils.MetricLogger(delimiter="  ")
    header = "Test:"
    coco = get_coco_api_from_dataset(data_loader.dataset)
    iou_types = ["bbox"]
    coco_evaluator = CocoEvaluator(coco, iou_types)
    count = 0
    idx = 0
    for images, targets in metric_logger.log_every(data_loader, 100, header):
        images, _ = images.decompose()

        images = list(img.to(device) for img in images)

        # resize the boxes in targets to integer coordinates
        for idxx, _ in enumerate(targets[0]['boxes']):
            targets[0]['boxes'][idxx][0] *= targets[0]['size'][1]
            targets[0]['boxes'][idxx][1] *= targets[0]['size'][0]
            targets[0]['boxes'][idxx][2] *= targets[0]['size'][1]
            targets[0]['boxes'][idxx][3] *= targets[0]['size'][0]

            targets[0]['boxes'][idxx] = box_cxcywh_to_xyxy(targets[0]['boxes'][idxx])
        targets[0]['labels'] += 1

        min_val = torch.min(images[0])
        max_val = torch.max(images[0])

        images[0] = (images[0] - min_val) * (1 / (max_val - min_val))

        if torch.cuda.is_available():
            torch.cuda.synchronize()
        model_time = time.time()
        outputs = model(images)
        outputs = filterOutput(outputs, threshold)
        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]
        model_time = time.time() - model_time

        # visualising the first image targets and the first image outputs
        choice = np.random.rand()
        outputs_visual = outputs
        if args.dataset_file == 'foos':
            outputs_visual[0]['labels'][outputs_visual[0]['labels'] > 4] = 0
            if torch.all(outputs_visual[0]['labels'] < 5) and idx < 2005 and choice < 1.002:
                # print('saving image...')
                drawTargets(images, targets, output_dir, count, nclasses)
                # drawResults(images, outputs_visual, output_dir, count, nclasses)
                idx += 1
        elif args.dataset_file == 'foos_positional':
            outputs_visual[0]['labels'][outputs_visual[0]['labels'] > 14] = 0
            if torch.all(outputs_visual[0]['labels'] < 15) and idx < 5 and choice < 0.002:
                print('saving image...')
                drawTargets(images, outputs_visual, output_dir, count, nclasses)
                # drawResults(images, outputs_visual, output_dir, count, nclasses)
                idx += 1
        else:
            print('INVALID DATASET FILE')
        # reformat outputs like in DETR version: 0:ball, 1:figure, 2:goal, 3:table
        # essentially do the inverse of the targets preprocessing
        # resize the boxes in targets to integer coordinates
        for idxx, _ in enumerate(outputs[0]['boxes']):
            outputs[0]['boxes'][idxx][0] *= targets[0]['orig_size'][1] / targets[0]['size'][1]
            outputs[0]['boxes'][idxx][1] *= targets[0]['orig_size'][0] / targets[0]['size'][0]
            outputs[0]['boxes'][idxx][2] *= targets[0]['orig_size'][1] / targets[0]['size'][1]
            outputs[0]['boxes'][idxx][3] *= targets[0]['orig_size'][0] / targets[0]['size'][0]
        outputs[0]['labels'] = outputs[0]['labels'] - 1

        res = {target["image_id"].item(): output for target, output in zip(targets, outputs)}

        evaluator_time = time.time()
        coco_evaluator.update(res)
        evaluator_time = time.time() - evaluator_time
        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)

        count += 1

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    # print("Averaged stats:", metric_logger)
    coco_evaluator.synchronize_between_processes()

    # accumulate predictions from all images
    coco_evaluator.accumulate()
    d = coco_evaluator.summarize()

    return coco_evaluator, d


def drawResults(imageList, outputs, output_dir, idx, nclasses):
    if nclasses == 4:
        classes = ['__background__', 'ball', 'figure', 'goal', 'table']
        colors = create_distinct_colors(len(classes))
        # random.Random(4).shuffle(colors)
    elif nclasses == 14:
        classes = ['__background__', 'ball', 'GK', 'RCB', 'LCB', 'RM', 'RCM', 'CM', 'LCM', 'LM', 'RW', 'ST', 'LW', 'goal', 'table']
        colors = create_distinct_colors(len(classes))
        # random.Random(4).shuffle(colors)
    else:
        print("No figures saved, number of classes doesn't exist")
        return
    threshold = 0

    for image, output in zip(imageList, outputs):
        boxes = output['boxes'][output['scores'] > threshold]
        labelsList = output['labels'][output['scores'] > threshold].tolist()
        colorsList = output['labels'][output['scores'] > threshold].tolist()
        labels = [classes[int(i)] for i in labelsList]
        colors = [colors[int(i)] for i in colorsList]
        f = plt.figure(figsize=(20, 20))
        plt.rcParams["savefig.bbox"] = 'tight'
        result = [draw_bounding_boxes((image * 255).to(torch.uint8), boxes=boxes, labels=labels, colors=colors, width=2,
                                      font_size=3) for image, output in zip(imageList, outputs)]

        plt.imshow(result[0].permute(1, 2, 0).to(torch.uint8))

        f.savefig(output_dir + str(idx) + 't.png')
        plt.close()

    return


def drawTargets(imageList, outputs, output_dir, idx, nclasses):
    if idx == 1:
        if nclasses == 4:
            classes = ['__background__', 'ball', 'figure', 'goal', 'table']
            colors = create_distinct_colors(len(classes))
            # random.Random(4).shuffle(colors)
        elif nclasses == 14:
            classes = ['__background__', 'ball', 'GK', 'RCB', 'LCB', 'RM', 'RCM', 'CM', 'LCM', 'LM', 'RW', 'ST', 'LW', 'goal', 'table']
            colors = create_distinct_colors(len(classes))
            # random.Random(4).shuffle(colors)
        else:
            print("No figures saved, number of classes doesn't exist")
            return

        for image, output in zip(imageList, outputs):
            boxes = output['boxes']  # [output['scores'] > threshold]
            labelsList = output['labels'].tolist()  # [output['scores'] > threshold].tolist()
            colorsList = output['labels'].tolist()  # [output['scores'] > threshold].tolist()
            labels = [classes[int(i)] for i in labelsList]
            colors = [colors[int(i)] for i in colorsList]
            # f = plt.figure(figsize=(20, 20))
            plt.rcParams["savefig.bbox"] = 'tight'
            result = [draw_bounding_boxes((image * 255).to(torch.uint8), boxes=boxes, labels=labels, colors=colors, width=2,
                                          font_size=3) for image, output in zip(imageList, outputs)]

            plt.imshow(result[0].permute(1, 2, 0).to(torch.uint8))
            # f.savefig(output_dir + str(idx) + 't.png')
            plt.axis('off')
            plt.savefig(output_dir + str(idx) + 't.png')
            plt.close()

    return


def filterOutput(outputs, threshold):
    output = outputs[0]
    newDict = {}
    for key in output:
        newDict[key] = output[key][output['scores'] >= threshold]
    return [newDict]

def main(args):
    model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False)
    weights = torch.jit.load(args.resume)
    model.load_state_dict(weights.state_dict())

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    dataset_test = build_dataset(image_set=args.image_set, args=args)

    sampler_test = torch.utils.data.SequentialSampler(dataset_test)

    data_loader_test = DataLoader(dataset_test, args.batch_size, sampler=sampler_test,
                                  drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)

    print("EVALUATING")
    results = []
    _ , res = evaluate(model,data_loader_test, device = device,threshold =args.thresh, output_dir=args.output_dir, nclasses=args.nclasses)
    results.append(res)

    file = open(args.output_dir + 'test.csv', 'w')
    writer = csv.writer(file, delimiter=',')
    header = ['MAP 0.5:95 all 100', 'MAP 0.5 all 100', 'MAP 0.75 all 100', 'MAP 0.5:95 small 100',
              'MAP 0.5:95 medium 100', 'MAP 0.5:95 large 100', 'Recall 0.5:95 all 1', 'Recall 0.5:95 all 10',
              'Recall 0.5:95 all 100', 'Recall 0.5:95 small 100', 'Recall 0.5:95 medium 1', 'Recall 0.5:95 large 100']
    writer.writerow(header)
    writer.writerows(results)
    file.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser('RetinaNet evaluation only script', parents=[get_args_parser()])
    args = parser.parse_args()
    if args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    main(args)

