"""
CODE FOR RUNNING DETR ON A FOLDER OF IMAGES - SAVES DETR LABELS AND A .CSV FILE WITH APS AND ARS

On Validation Set, 4 classes:
python detr_experiments/test_detr.py --data_path ../Segmentsai/Test --resume ../500epochweights/checkpoint.pth --dataset_file foos --thresh 0.7 --output_dir detr_experiments/images/ --image_set val
On Synthetic data, 4 classes:
python detr_experiments/test_detr.py --data_path ../Segmentsai/synthesiser/data --resume ../500epochweights/checkpoint.pth --dataset_file foos --thresh 0.7 --output_dir detr_experiments/images/ --image_set test


On validation data, 14 classes:
python detr_experiments/test_detr.py --data_path ../Segmentsai/Test --resume ../600epoch_positional2/checkpoint.pth --dataset_file foos_positional --thresh 0.5 --output_dir detr_experiments/images/ --image_set val --nclasses 14
On synthetic data, 14 classes:
python detr_experiments/test_detr.py --data_path ../Segmentsai/synthesiser/data --resume ../600epoch_positional2/checkpoint.pth --dataset_file foos_positional --thresh 0.5 --output_dir detr_experiments/test_images/ --image_set test --nclasses 14



"""
"""
CODE FOR TESTING RETINANET, AND ALSO VISUALISING THE TARGETS OF A DATASET.

On Validation Set, 4 classes:
python RetinaNet/test_retinanet.py --data_path ../Segmentsai/Test --resume RetinaNet/retinanet_output/model_scripted_original.pt --dataset_file foos --thresh 0.7 --output_dir images/ --image_set val
On Synthetic data, 4 classes:
python RetinaNet/test_retinanet.py --data_path ../Segmentsai/synthesiser/data --resume RetinaNet/retinanet_output/model_scripted_original.pt --dataset_file foos --thresh 0.5 --output_dir RetinaNet/test_images/ --image_set test

On Validation set, 14 classes:
python RetinaNet/test_retinanet.py --data_path ../Segmentsai/Test --resume RetinaNet/retinanet_output/model_scripted.pt --dataset_file foos_positional --thresh 0.5 --output_dir RetinaNet/images_pos/ --image_set val --nclasses 14


"""

import math
import os
import cv2
import sys
import argparse
from pathlib import Path
from torch.utils.data import DataLoader
from typing import Iterable
from PIL import Image
import numpy as np
import torch
import torchvision
import torch.utils
from torchvision.utils import draw_bounding_boxes
import matplotlib.pyplot as plt
import time
import csv
import random
from coco_eval import CocoEvaluator

filedir = os.path.join("..", os.getcwd())
sys.path.append(filedir)
import util.misc as utils
from models import build_model
from datasets.foos import make_Foos_transforms
from engine import evaluate
from datasets import build_dataset, get_coco_api_from_dataset

import colorsys

def create_distinct_colors(n):
    colors = []
    for i in range(n):
        hue = i / n  # Vary the hue value evenly
        saturation = 0.7  # Adjust the saturation and lightness as needed
        lightness = 0.6
        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)
        rgb_scaled = tuple(int(x * 255) for x in rgb)  # Scale RGB values to 0-255
        colors.append(rgb_scaled)
    return colors

def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=1)

def rescale_bboxes(out_bbox, size):
    img_w, img_h = size
    b = box_cxcywh_to_xyxy(out_bbox)
    b = b * torch.tensor([img_w, img_h,
                          img_w, img_h
                          ], dtype=torch.float32)
    return b

def get_images(in_path):
    img_files = []
    # for (dirpath, dirnames, filenames) in os.walk(in_path):
    #     for file in filenames:
    #         filename, ext = os.path.splitext(file)
    #         ext = str.lower(ext)
    #         if ext == '.jpg' or ext == '.jpeg' or ext == '.gif' or ext == '.png' or ext == '.pgm':
    #             img_files.append(os.path.join(dirpath, file))

    for i in range(len(os.listdir(in_path))):
        filename = str(i) + '.png'
        img_files.append(os.path.join(in_path, filename))

    return img_files

def get_args_parser():
    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)
    parser.add_argument('--batch_size', default=1, type=int)

    # * Segmentation
    parser.add_argument('--masks', action='store_true',
                        help="Train segmentation head if the flag is provided")
    # dataset parameters
    parser.add_argument('--dataset_file', default='foos_positional')
    parser.add_argument('--data_path', default='C:/Users/OliverSchamp/Documents/Thesis/Segmentsai/',
                        type=str)  # changed from --coco_path
    parser.add_argument('--output_dir', default='retinanet_output/', type=str,
                        help='path where to save, empty for no saving')
    parser.add_argument('--device', default='cuda',
                        help='device to use for training / testing')
    parser.add_argument('--seed', default=42, type=int)
    parser.add_argument('--num_workers', default=2, type=int)

    parser.add_argument('--nclasses', default=4, type=int)
    parser.add_argument('--image_set', default='', type=str)
    parser.add_argument('--thresh', default=0.7, type=float)
    parser.add_argument('--resume', default='', help='resume from checkpoint')

    return parser

def drawResults(imageList, outputs, output_dir, idx, nclasses):
    if nclasses == 4:
        classes = ['__background__', 'ball', 'figure', 'goal', 'table']
        colors = create_distinct_colors(len(classes))
        # random.Random(4).shuffle(colors)
    elif nclasses == 14:
        classes = ['__background__', 'ball', 'GK', 'RCB', 'LCB', 'RM', 'RCM', 'CM', 'LCM', 'LM', 'RW', 'ST', 'LW', 'goal', 'table']
        colors = create_distinct_colors(len(classes))
        # random.Random(4).shuffle(colors)
    else:
        print("No figures saved, number of classes doesn't exist")
        return
    threshold = 0

    for image, output in zip(imageList, outputs):
        boxes = output['boxes'][output['scores'] > threshold]
        labelsList = output['labels'][output['scores'] > threshold].tolist()
        colorsList = output['labels'][output['scores'] > threshold].tolist()
        labels = [classes[int(i)] for i in labelsList]
        colors = [colors[int(i)] for i in colorsList]
        f = plt.figure(figsize=(20, 20))
        plt.rcParams["savefig.bbox"] = 'tight'
        result = [draw_bounding_boxes((image).permute(2,0,1).to(torch.uint8), boxes=boxes, labels=labels, colors=colors, width=2,
                                      font_size=3) for image, output in zip(imageList, outputs)]

        plt.imshow(result[0].permute(1, 2, 0).to(torch.uint8))
        plt.axis('off')
        f.savefig(output_dir + str(idx) + 't.png')
        plt.close()

    return

def filterOutput(outputs, threshold):
    output = outputs[0]
    newDict = {}
    for key in output:
        newDict[key] = output[key][output['scores'] >= threshold]
    return [newDict]

@torch.no_grad()
def infer(images_path, model, device, args):
    model.eval()
    cpu_device = torch.device("cpu")
    for idx, img_sample in enumerate(images_path):
        if idx % 10 == 0:
            filename = os.path.basename(img_sample)
            print("processing...{}".format(filename))
        orig_image = Image.open(img_sample, mode='r')
        # w, h = orig_image.size
        # transform = make_Foos_transforms("val")
        # dummy_target = {
        #     "size": torch.as_tensor([int(h), int(w)]),
        #     "orig_size": torch.as_tensor([int(h), int(w)])
        # }
        # image, targets = transform(orig_image, dummy_target)
        # image = image.unsqueeze(0)

        image = torch.from_numpy(np.array(orig_image)).permute(2,0,1)
        image = image / 255
        image = image.unsqueeze(0)
        image = image.to(device)

        model_time = time.time()
        outputs = model(image)
        outputs = filterOutput(outputs, args.thresh)
        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]
        model_time = time.time() - model_time

        imag_array = torch.from_numpy(np.array(orig_image))

        drawResults([imag_array], outputs, args.output_dir, idx, args.nclasses)


def main(args):
    model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False)
    weights = torch.jit.load(args.resume)
    model.load_state_dict(weights.state_dict())

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    image_paths = get_images(args.data_path)
    # random.shuffle(image_paths)
    infer(image_paths, model, device, args)


if __name__ == "__main__":
    parser = argparse.ArgumentParser('RetinaNet non-evaluation, only viewing script', parents=[get_args_parser()])
    args = parser.parse_args()
    if args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    main(args)