# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

#To run, 4 classes, synthetic data:
#python detr_experiments/visualise_decoder_attention.py --data_path C:\Users\OliverSchamp\Documents\Thesis\Segmentsai\synthesiser\data --resume ../500epochweights/checkpoint.pth --dataset_file foos --sine_input_csv C:\Users\OliverSchamp\Documents\Thesis\Segmentsai\synthesiser\sine_inputs.csv
# python detr_experiments/visualise_decoder_attention.py --data_path C:\Users\OliverSchamp\Documents\Thesis\Segmentsai\Test --resume ../600epoch_positional2/checkpoint.pth --dataset_file foos_positional --sine_input_csv C:\Users\OliverSchamp\Documents\Thesis\Segmentsai\synthesiser\sine_inputs.csv


import math
import os
import cv2
import sys
import argparse
from pathlib import Path
from typing import Iterable
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import time
import torch
import random
import pandas as pd

filedir = os.path.join("..", os.getcwd())
sys.path.append(filedir)
import util.misc as utils
from models import build_model
from datasets.foos_positional import make_Foos_transforms

CLASSES = [
    "ball", "fig", "goal", "table"
]

def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=1)

def rescale_bboxes(out_bbox, size):
    img_w, img_h = size
    b = box_cxcywh_to_xyxy(out_bbox)
    b = b * torch.tensor([img_w, img_h,
                          img_w, img_h
                          ], dtype=torch.float32)
    return b

# the image function must be changed to the below if calculating the decoder attention on synthetic data
def get_images(in_path):
    img_files = []
    # for (dirpath, dirnames, filenames) in os.walk(in_path):
    #     for file in filenames:
    #         filename, ext = os.path.splitext(file)
    #         ext = str.lower(ext)
    #         if ext == '.jpg' or ext == '.jpeg' or ext == '.gif' or ext == '.png' or ext == '.pgm':
    #             img_files.append(os.path.join(dirpath, file))

    for i in range(len(os.listdir(in_path))):
        filename = str(i) + '.png'
        img_files.append(os.path.join(in_path, filename))

    return img_files


def get_args_parser():
    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)

    #default command line arguments
    parser.add_argument('--lr', default=1e-4, type=float) #learning rate of 1e-4
    parser.add_argument('--lr_backbone', default=1e-5, type=float)
    parser.add_argument('--batch_size', default=6, type=int)
    parser.add_argument('--weight_decay', default=1e-4, type=float)
    parser.add_argument('--epochs', default=300, type=int)
    parser.add_argument('--lr_drop', default=200, type=int)
    parser.add_argument('--clip_max_norm', default=0.1, type=float,
                        help='gradient clipping max norm')

    # Model parameters
    parser.add_argument('--frozen_weights', type=str, default=None,
                        help="Path to the pretrained model. If set, only the mask head will be trained")
    # * Backbone
    parser.add_argument('--backbone', default='resnet50', type=str,
                        help="Name of the convolutional backbone to use")
    parser.add_argument('--dilation', action='store_true',
                        help="If true, we replace stride with dilation in the last convolutional block (DC5)")
    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),
                        help="Type of positional embedding to use on top of the image features")

    # * Transformer
    parser.add_argument('--enc_layers', default=6, type=int,
                        help="Number of encoding layers in the transformer")
    parser.add_argument('--dec_layers', default=6, type=int,
                        help="Number of decoding layers in the transformer")
    parser.add_argument('--dim_feedforward', default=2048, type=int,
                        help="Intermediate size of the feedforward layers in the transformer blocks")
    parser.add_argument('--hidden_dim', default=256, type=int,
                        help="Size of the embeddings (dimension of the transformer)")
    parser.add_argument('--dropout', default=0.1, type=float,
                        help="Dropout applied in the transformer")
    parser.add_argument('--nheads', default=8, type=int,
                        help="Number of attention heads inside the transformer's attentions")
    parser.add_argument('--num_queries', default=29, type=int,
                        help="Number of query slots")
    parser.add_argument('--pre_norm', action='store_true')

    # * Segmentation
    parser.add_argument('--masks', action='store_true',
                        help="Train segmentation head if the flag is provided")

    # # Loss
    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',
                        help="Disables auxiliary decoding losses (loss at each layer)")
    # * Matcher
    parser.add_argument('--set_cost_class', default=1, type=float,
                        help="Class coefficient in the matching cost")
    parser.add_argument('--set_cost_bbox', default=5, type=float,
                        help="L1 box coefficient in the matching cost")
    parser.add_argument('--set_cost_giou', default=2, type=float,
                        help="giou box coefficient in the matching cost")
    # * Loss coefficients
    parser.add_argument('--mask_loss_coef', default=1, type=float)
    parser.add_argument('--dice_loss_coef', default=1, type=float)
    parser.add_argument('--bbox_loss_coef', default=5, type=float)
    parser.add_argument('--giou_loss_coef', default=2, type=float)
    parser.add_argument('--eos_coef', default=0.1, type=float,
                        help="Relative classification weight of the no-object class")

    # dataset parameters
    parser.add_argument('--dataset_file', default='foos')
    parser.add_argument('--data_path', type=str, default=r"C:\Users\OliverSchamp\Documents\Thesis\Segmentsai")
    parser.add_argument('--data_panoptic_path', type=str)
    parser.add_argument('--remove_difficult', action='store_true')

    parser.add_argument('--output_dir', default='images/',
                        help='path where to save the results, empty for no saving')
    parser.add_argument('--device', default='cuda',
                        help='device to use for training / testing')
    parser.add_argument('--resume', default='../../500epochweights/checkpoint.pth', help='resume from checkpoint')

    parser.add_argument('--thresh', default=0.5, type=float)

    parser.add_argument('--sine_input_csv', default='', type=str)

    return parser

#TODO: remove null query attention maps
@torch.no_grad()
def infer(images_path, model, postprocessors, device, output_path):
    interval = 5
    model.eval()
    decoder_maps = np.zeros((0,) + (29,) + (1008,), dtype=np.float32)
    for hh, img_sample in enumerate(images_path):
        filename = os.path.basename(img_sample)
        if hh % 10 == 0:
            print("processing...{}".format(filename))

        if int(filename.split('.')[0]) % interval != 0:
            continue

        orig_image = Image.open(img_sample, mode='r')
        w, h = orig_image.size
        transform = make_Foos_transforms('val')
        dummy_target = {
            "size": torch.as_tensor([int(h), int(w)]),
            "orig_size": torch.as_tensor([int(h), int(w)])
        }
        image, targets = transform(orig_image, dummy_target)
        image = image.unsqueeze(0)
        image = image.to(device)


        conv_features, enc_attn_weights, dec_attn_weights = [], [], []
        hooks = [
            model.backbone[-2].register_forward_hook(
                        lambda self, input, output: conv_features.append(output)

            ),
            model.transformer.encoder.layers[-1].self_attn.register_forward_hook(
                        lambda self, input, output: enc_attn_weights.append(output[1])

            ),
            model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(
                        lambda self, input, output: dec_attn_weights.append(output[1])

            ),

        ]

        outputs = model(image)

        outputs["pred_logits"] = outputs["pred_logits"].cpu()
        outputs["pred_boxes"] = outputs["pred_boxes"].cpu()

        probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]
        # keep = probas.max(-1).values > 0.85
        keep = probas.max(-1).values > 0 #args.thresh # only visualise the probabilities over a certain threshold
        remove = probas.max(-1).values < 0 #if you want to set parts of the decoder attention maps to 0 below a certain threshold

        for hook in hooks:
            hook.remove()

        conv_features = conv_features[0]
        enc_attn_weights = enc_attn_weights[0]
        dec_attn_weights = dec_attn_weights[0].cpu()

        #visualising cross-attention --------------------------
        # get the feature map shape
        h, w = conv_features['0'].tensors.shape[-2:]


        dec_attn_weights[0, remove] = 0 #threshold attention maps
        if args.dataset_file == 'foos':
            dec_attn_weights[0, probas.argmax(-1) == 4] = 0 #remove nulls
        if args.dataset_file == 'foos_positional':
            dec_attn_weights[0, probas.argmax(-1) == 14] = 0  # remove nulls
        #showing the differences between consecutive images
        decoder_maps = np.append(decoder_maps, dec_attn_weights, axis=0)

        bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], orig_image.size)
        probas = probas[keep].cpu().data.numpy()

        for idx, box in zip(keep.nonzero(), bboxes_scaled):
            img3 = np.array(dec_attn_weights[0, idx].view(h, w))
            img3 = ((img3 - img3.min()) / (img3.max() - img3.min()))
            scale_percent = 1000  # percent of original size
            width = int(img3.shape[1] * scale_percent / 100)
            height = int(img3.shape[0] * scale_percent / 100)
            dim = (width, height)
            img3 = cv2.resize(img3, dim, interpolation=cv2.INTER_AREA)
            # img_save_path = os.path.join(output_path, str(np.array(idx)[0])+filename.split('.')[0]+'decoder'+'.jpg')
            img_save_path = os.path.join(output_path, str(int(filename.split('.')[0])//interval)+'.png')

            bbox = box.cpu().data.numpy()
            bbox = bbox.astype(np.int32)
            bbox = np.array([
                [bbox[0], bbox[1]],
                [bbox[2], bbox[1]],
                [bbox[2], bbox[3]],
                [bbox[0], bbox[3]],
                ])
            bbox = bbox.reshape((4, 2))

            orig_image2 = cv2.cvtColor(np.array(orig_image), cv2.COLOR_BGR2RGB)
            attn_map = cv2.resize(img3, (orig_image2.shape[1], orig_image2.shape[0]), interpolation = cv2.INTER_AREA)
            overlay = np.zeros(orig_image2.shape)
            overlay[:, :] = (0, 0, 255)  # Set color to red
            overlay[:, :, 2] = overlay[:, :, 2] * attn_map
            overlay = overlay.astype(np.uint8)
            output = cv2.addWeighted(orig_image2, 0.2, overlay, 3, 0)
            cv2.polylines(output, [bbox], True, (0, 255, 0), 2)
            # cv2.imshow("test", output)
            # cv2.waitKey()
            if idx == 18: #only do this if the threshold is set to 0 - used for presentation visualisations
                cv2.imwrite(img_save_path, output)

    # DECODER SCATTER PLOTS AND AVERAGE ATTENTION MAPS


    #random to compare #START COMMENT
    # # random_maps = np.random.rand(decoder_maps.shape[0], decoder_maps.shape[1], decoder_maps.shape[2])
    # #remove the really high attentions
    # decoder_maps_thresholded = np.copy(decoder_maps)
    # decoder_maps_thresholded[decoder_maps > 0.1] = 0
    # #shuffle the decoder maps
    # decoder_maps_thresholded_shuffle = np.copy(decoder_maps_thresholded)
    # random.shuffle(decoder_maps_thresholded_shuffle)
    #
    # #load the sine inputs from the synthetic data
    # sine_inputs = pd.read_csv(args.sine_input_csv)
    # distance = [sum(sum(abs(decoder_maps[i-1, :, :]-decoder_maps[i, :, :]))) for i in range(1, decoder_maps.shape[0])]
    # distance_thresh = [sum(sum(abs(decoder_maps_thresholded[i - 1, :, :] - decoder_maps_thresholded[i, :, :]))) for i in range(1, decoder_maps_thresholded.shape[0])]
    # correlation = np.corrcoef(distance, distance_thresh)[0, 1]
    # print("Correlation: ", correlation)
    # distance_thresh_shuf = [sum(sum(abs(decoder_maps_thresholded_shuffle[i - 1, :, :] - decoder_maps_thresholded_shuffle[i, :, :]))) for i in range(1, decoder_maps_thresholded_shuffle.shape[0])]
    # correlation = np.corrcoef(distance, distance_thresh_shuf)[0, 1]
    # print("Correlation, shuffled: ", correlation)
    #
    # #randomly sampling two sine configurations and the corresponding attention maps - SCATTER PLOTS
    # a = []
    # aa = []
    # for i in range(1000):
    #     choice1 = np.random.choice(2000)
    #     choice2 = np.random.choice(2000)
    #     while choice2 == choice1:
    #         choice2 = np.random.choice(2000)
    #
    #     sine1 = sine_inputs.iloc[choice1].values
    #     sine2 = sine_inputs.iloc[choice2].values
    #     avg_dist = np.mean(np.abs(sine1-sine2))
    #
    #     a.append(avg_dist)
    #     dist = sum(sum(abs(decoder_maps[choice1, :, :] - decoder_maps[choice2, :, :])))
    #     aa.append(dist)
    #
    # plt.scatter(a, aa)
    # # Apply logarithmic transformation to x-values
    # log_x = np.log(a)
    #
    # # Calculate the logarithmic fit using polyfit
    # degree = 1  # Degree of the polynomial regression line (1 for a straight line)
    # coefficients = np.polyfit(log_x, aa, degree)
    # line = np.poly1d(coefficients)
    #
    # # Generate x values for plotting the line
    # x_line = np.linspace(min(log_x), max(log_x), 100)
    #
    # # Plot the logarithmic fit
    # plt.plot(np.exp(x_line), line(x_line), color='red', label='y={:.2f}log(x)+{:.2f}'.format(line[1], line[0]))
    # plt.xlabel('Average distance between synthetic configurations')
    # plt.ylabel('Total distance between attention maps')
    # plt.title('How differences in structure affect DETR attention maps')
    # plt.legend(loc="lower right")
    # plt.ylim(0, plt.ylim()[1])
    # # plt.show()
    # plt.savefig("detr_experiments/experiments_figs/attnmaps_4c.png", bbox_inches="tight")
    # plt.close()
    #
    # a = []
    # aa = []
    # for i in range(1000):
    #     choice1 = np.random.choice(2000)
    #     choice2 = np.random.choice(2000)
    #     while choice2 == choice1:
    #         choice2 = np.random.choice(2000)
    #
    #     sine1 = sine_inputs.iloc[choice1].values
    #     sine2 = sine_inputs.iloc[choice2].values
    #     avg_dist = np.mean(np.abs(sine1-sine2))
    #     a.append(avg_dist)
    #     dist = sum(sum(abs(decoder_maps_thresholded[choice1, :, :] - decoder_maps_thresholded[choice2, :, :])))
    #     aa.append(dist)
    #
    # plt.scatter(a, aa)
    # # Apply logarithmic transformation to x-values
    # log_x = np.log(a)
    #
    # # Calculate the logarithmic fit using polyfit
    # degree = 1  # Degree of the polynomial regression line (1 for a straight line)
    # coefficients = np.polyfit(log_x, aa, degree)
    # line = np.poly1d(coefficients)
    #
    # # Generate x values for plotting the line
    # x_line = np.linspace(min(log_x), max(log_x), 100)
    #
    # # Plot the logarithmic fit
    # plt.plot(np.exp(x_line), line(x_line), color='red', label='y={:.2f}log(x)+{:.2f}'.format(line[1], line[0]))
    # plt.xlabel('Average distance between synthetic configurations')
    # plt.ylabel('Total distance between attention maps')
    # plt.title('How differences in structure affect DETR attention maps')
    # plt.legend(loc="lower right")
    # plt.ylim(0, plt.ylim()[1])
    # # plt.show()
    # plt.savefig("detr_experiments/experiments_figs/attnmaps_4c_thresh.png", bbox_inches="tight")
    # plt.close()
    #
    # a = []
    # aa = []
    # for i in range(1000):
    #     choice1 = np.random.choice(2000)
    #     choice2 = np.random.choice(2000)
    #     while choice2 == choice1:
    #         choice2 = np.random.choice(2000)
    #
    #     sine1 = sine_inputs.iloc[choice1].values
    #     sine2 = sine_inputs.iloc[choice2].values
    #     avg_dist = np.mean(np.abs(sine1-sine2))
    #     a.append(avg_dist)
    #     dist = sum(sum(abs(decoder_maps_thresholded_shuffle[choice1, :, :] - decoder_maps_thresholded_shuffle[choice2, :, :])))
    #     aa.append(dist)
    #
    # plt.scatter(a, aa)
    # # Apply logarithmic transformation to x-values
    # log_x = np.log(a)
    #
    # # Calculate the logarithmic fit using polyfit
    # degree = 1  # Degree of the polynomial regression line (1 for a straight line)
    # coefficients = np.polyfit(log_x, aa, degree)
    # line = np.poly1d(coefficients)
    #
    # # Generate x values for plotting the line
    # x_line = np.linspace(min(log_x), max(log_x), 100)
    #
    # # Plot the logarithmic fit
    # plt.plot(np.exp(x_line), line(x_line), color='red', label='y={:.2f}log(x)+{:.2f}'.format(line[1], line[0]))
    # plt.xlabel('Average distance between synthetic configurations')
    # plt.ylabel('Total distance between attention maps')
    # plt.title('How differences in structure affect DETR attention maps')
    # plt.legend(loc="lower right")
    # plt.ylim(0, plt.ylim()[1])
    # # plt.show() #END COMMENT
    # plt.savefig("detr_experiments/experiments_figs/attnmaps_4c_shuffled.png", bbox_inches="tight")
    # plt.close()
    #
    #
    # #AVERAGE ATTENTION MAPS
    # for j in range(decoder_maps.shape[1]):
    #     query_map = np.zeros(decoder_maps[0,0,:].shape)
    #     for i in range(decoder_maps.shape[0]):
    #         query_map += decoder_maps[i, j, :] #for a chosen query j
    #
    #     query_map /= decoder_maps.shape[0]
    #     query_map = ((query_map - query_map.min()) / (query_map.max() - query_map.min()))
    #     query_map = np.reshape(query_map, (h, w))
    #     query_map = ((query_map - query_map.min()) / (query_map.max() - query_map.min()))
    #     mid_img_path = os.path.join(args.data_path, '999.png')
    #     mid_image = Image.open(mid_img_path, mode='r')
    #     mid_image2 = cv2.cvtColor(np.array(mid_image), cv2.COLOR_BGR2RGB)
    #     attn_map = cv2.resize(query_map, (mid_image2.shape[1], mid_image2.shape[0]), interpolation = cv2.INTER_AREA)
    #     overlay = np.zeros(mid_image2.shape)
    #     overlay[:, :] = (0, 0, 255)  # Set color to red
    #     overlay[:, :, 2] = overlay[:, :, 2] * attn_map
    #     overlay = overlay.astype(np.uint8)
    #     output = cv2.addWeighted(mid_image2, 0.2, overlay, 10, 0)
    #
    #     cv2.imwrite("detr_experiments/Avg_attns_4c/Query{}.png".format(j), output)

if __name__ == "__main__":
    parser = argparse.ArgumentParser('DETR decoder cross attention script', parents=[get_args_parser()])
    args = parser.parse_args()
    if args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)

    device = torch.device(args.device)

    model, _, postprocessors = build_model(args)
    if args.resume:
        checkpoint = torch.load(args.resume, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
    model.to(device)
    image_paths = get_images(args.data_path)
    # random.shuffle(image_paths)
    infer(image_paths, model, postprocessors, device, args.output_dir)